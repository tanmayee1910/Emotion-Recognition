{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Emotion Recognition from Facial Expressions using Deep leaning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facial Emotion Recognition is a basis of human interface technology with applications in society ranging from social robots to video games that modify difficulty levels based on the player's facial expressions. Many academics use deep learning approaches to develop models with higher accuracy, particularly Convolutional Neural Networks (CNN), which excel at pattern recognition and image processing. To improve the accuracy of the dataset, this study uses advanced Efficientnet architecture (FER2013). When compared to Resnet and the VGG16 model, Efficientnet, one of the proposed designs, has the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stimulation of particular facial muscles allows humans to transmit their emotions through their faces. These expressions are sometimes accurate, but they can be difficult to grasp; signals in a face expression can contain a wealth of information about their emotional state. Humans are generally adept at recognizing and comprehending the emotions of others. “Anger,“, “contempt, “,“disgust, “,  “fear, “,“happiness,”, “sadness, “, and “surprise,“ are among the seven  emotions of human face that can be roughly classified. This project will use Python to implement the source code for a facial expression recognition system. We'll use the deep neural network to solve complicated issues like facial expression detection. In today's world of computer vision, facial expression detection systems are extremely important. \n",
    "\n",
    "It helps to comprehend the underlying meaning of human-machine interaction, which is useful in a variety of applications such as diagnosing mental diseases, assessing mental states, detecting lies, and so on. The task of accurately recognising human expressions remains difficult. Deep learning techniques, which are particularly successful for image processing, are used here. The FER2013 dataset is used for analysis, and popular “Deep Learning (DL)“ frameworks for example OpenCV and Efficientnet are employed to accurately detect facial emotions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As model and training data sizes grow bigger, deep network training performance gets extremely important. GPT-3 (Brown et al., 2020), for instance, demonstrates outstanding capabilities in few class classification from a much larger size input data and more training data, but still it takes many days of work with dozens of 'GPUs' that building  problematic toward retraining. Consequently, training effectiveness have received a lot of attention. For illustration, NFNets (Brock et al., 2021) intends to enhance subsequent revisions by trying to reduce the time-consuming convolution layers. Some researcher ((Srinivas et al., 2021) works to improve learning rate via incorporating responsiveness network layers into deep convolutional; and Vision Transformers (Dosovitskiy et al., 2021) utilizes Transformer components to enhance training efficiency on huge datasets. These methods, however, constantly come with many pitfalls.\n",
    "\n",
    "Techniques frequently come from high cost whenever dealing with large feature sizes (b). On increase all learning rate and feature efficiency, designers integrate training-aware neural architecture search (NAS) alongside scalability inside this work. We start by systematically analyzing the learning barriers in EfficientNets, considering its feature efficiency (Tan & Le, 2019a). In EfficientNets, we noticed below points, \n",
    "\n",
    "(1)\tLearning rate for large image sizes is slow, \n",
    "\n",
    "(2)\tDepthwise convolution layers in previous layer are slow. \n",
    "\n",
    "(3) Correspondingly speeding up every stage is expensive. \n",
    "\n",
    "Findings from this study, we construct a search area which includes additional procedures like 'Fused-MBConv' method, then apply 'training-aware- NAS' then scale for optimize training model accuracy, speed of the training process, as well as input feature/parameter size simultaneously.  \n",
    "\n",
    "Researchers discovered systems, called EfficientNetV2, can train the model up to four times quicker than traditional methods, by using 6.8 times fewer features. They can speed it up our training with gradually raising the image size while doing it. Several past studies have utilized lower image sizes in training, like (Howard, 2018) progressive resizing, (Touvron et al., 2019) FixRes, as well as (Hoffer et al., 2019) Mix & Match approaches. However, they typically maintain the same training process for all image sizes, which leads to loss of accuracy. Researchers suggest that using same parameterization/ training process  for various image sizes is not perfect: small image size result in smaller network capacity and so needs small training data; high image size, on either hand, requires stronger training data to prevent fitting problem. Researchers develop an enhanced dynamic learning method based on this understanding. They training the model with lesser input image sizes also thin normalization (for example, loss as well as data augmentation) in the initial training epochs, then gradually increase size of the image and add deeper regularization. \n",
    "\n",
    "Their strategy relies on continuous scaling (Howard, 2018), but can increase speed training without sacrificing quality by constantly adjusting regularisation. The advanced EfficientNetV2 accomplishes robust performance on various image dataset such as CIFAR100, ImageNet, Cars, CIFAR-10 and Flowers models since for faster progressive learning. They achieved 85.7 % of accuracy on ImageNet despite training the model with 3 times to 9times faster and being 6.8x less than earlier models. Also it's easier to train systems on huge datasets with our EfficientNetV2 and progressive learning. \n",
    "\n",
    "As instance, ImageNet21k (Russakovsky et al., 2015) is roughly 10 times greater as ImageNet ILSVRC2012, however the EfficientNetV2 could accomplish the train in couple of days only with 32 TPUv3 cores. The EfficientNetV2 obtains 87.3% first rank accuracy on 'ImageNet- ILSVRC2012' dataset, which following learning methodologies on the available ImageNet21k, beating the other method such as recent 'ViT-L/16' by 2.0 % error rate while training 5 times to 11 times faster. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We consider 3 contributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tEfficientNetV2, a young family of shorter as well as faster models, is presented. EfficientNetV2 improves previous systems in terms of training time and feature optimization, due to its training-aware NAS and scaling.\n",
    "\n",
    "•\tThis work proposed an upgraded continuous active learning system, which changes parameterization and image size flexibly. We demonstrate that this really enhances accuracy even while accelerating training. \n",
    "\n",
    "•\tWe show up to 11 times faster training speed also up to 6.8 times  improved feature optimization efficiency on existing dataset such ImageNet, CIFAR, Cars, and Flowers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside this section, we examine the EfficientNet's 'training constraints' and its 'training-aware NAS plus scaling' and 'EfficientNetV2' algorithms (Tan & Le, 2019a). This author established a collection of methods which are tuned for 'FLOPs' then feature/parameter optimization. This system exploits NAS technique to determine basis of 'EfficientNet-B0', which has a good balance of accuracy with 'FLOPs' model and the initial system is then enlarged using a complex strategy in scaling process to generate the B1-B7 paradigm group. Despite recent reports of big changes in training but rather test time, this system are regularly weaker than EfficientNet method in terms of features/parameters and effectiveness of FLOPs process.  Our goal of this work is to improve learning rate simultaneously maintaining feature quality.\n",
    "Researchers evaluate the learning constraints of EfficientNet (Tan & Le, 2019a), that will be referred to as EfficientNetV1 from now on, as well as a few simple techniques for improving learning rate. Earlier studies have shown EfficientNet's (Radosavovic et al., 2020) the big image size causes foremost memory consumption. Researchers have to train such systems with smaller batches so because overall memory on GPU/TPU is limited, that significantly starts to slow down the training. FixRes is (Touvron et al., 2019) an easily enhancement network that utilizes a smaller size of input image for training process than for inference process.  Low resolution image sizes lead to fewer calculations and enable for higher batch sizes, improving learning rate by up to 2.2 times. According to Brock et al. (2021), employing a lower size of the image for learning improves the accuracy significantly. We do not adjust any layers after training, unlike (Touvron et al., 2019).\n",
    "\n",
    "EfficientNet's large depth - wise separable convolution layers are just another training issue (Sifre, 2014). While depth - wise separable convolution layers used less parameters and FLOPs over regular convolution layers, they can't usually make utilisation modern processors. To better leverage portable or server accelerators, Fused-MBConv suggested by Gupta & Tan (2019) and this method was used by Gupta & Akin (2020), Xiong et al., (2020) and Li et al., (2021). Sandler et al (2018) and Tan & Le, (2019) substitutes the depthwise conv3x3 and expansion conv1x1 with a single normal conv3x3 by MBConv method. \n",
    "\n",
    "This work progressively update the old MBConv in EfficientNet-B4 by FusedMBConv to evaluate the two basic components. FusedMBConv can increase training learning rate with a small impact on input features and FLOPs when it is used in preliminary phase 1-3, but since all units are replaced by the Fused-MBConv model (stage 1-7), which considerably rises features/parameters as well as the slowing down the learning process by using FLOPs. Determine the best combination/hybrid of different basic components, MBConv and Fused-MBConv, is hard, that's why we adopted neural network models investigation to find the optimum combination automatically.\n",
    "\n",
    "Using only a simple composite scale rule, EfficientNet properly expands all stages. Whenever the depth coefficient is 2, for examples, the number of layers in all phases of the system increases. However, but not all of those stages make contributions to train rate and feature effectiveness. We'll employ a – anti scaling strategy inside this work to progressively add additional layers to later steps. EfficientNets often forcefully scale up size of the image that process primary reason for huge memory utilization also it slow the training process. This issues are solved by the scaling the rule significantly also confine the maximal size of the input image to a lower rate.\n",
    "They utilize similar compounds scaling as (Tan & Le, 2019a) to massively increase 'EfficientNetV2-S' to get 'EfficientNetV2-M/L', with some adjustments. This work gradually add additional network layers to final phases to boost network infrastructure without imposing much execution complexity.\n",
    "\n",
    "They exhibit the following curves for EfficientNet, one can be trained (Tan & Le, 2019a) by both the normal inference size and then another one is trained by a slightly smaller size of image, comparable to (Touvron et al., 2019) EfficientNetV2 system and (Brock et al., 2021) NFNet method. With the exception of NFNets, that are trained using 360 epochs, most model can be trained using 350 epochs, culminating in an equal amount of training cycles. Amazingly, they discover so when properly trained, EfficientNets typically ensure a better achievement ratio. Most importantly, our suggested EfficientNetV2 system trains significantly faster than in other recent models due to its training-aware NAS & scaling. Those results correspond previous inference conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition requirements and specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tJupyter is used to run the planned system, which includes Anaconda 3, OpenCV, and pytorch.\n",
    "\n",
    "•\tThe entire experiment is run on an 8GB of RAM with “Intel(R)- Core(TM)-i5-3230M processor“   CPU machine with and a 64-bit Windows Operating System.\n",
    "\n",
    "•\tUse pip install comments in -r requirements.txt for installing the all required packages for this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of project work is to improve a DL- established scheme for understanding people's emotions that can take human facial images and recognize and classify them into seven different expressions. The main goal is to collect an image using FER and then use Deep leaning methods to detect the emotion of the person present from the input photographs. \n",
    "\n",
    "•\tCreate a solid framework for recognizing “happy,“, “sad,“, “surprise,“, “fear,“, “rage,“, “disgust,“, and neutral facial expressions automatically.\n",
    "\n",
    "•\tTo research and apply existing methods for identifying human face expressions, such as Resnet and VGG16.\n",
    "\n",
    "•\tTo use an advanced deep learning algorithm to identifying human face expressions with a high level of accuracy value and efficiency.\n",
    "\n",
    "•\tTo compare and assess the recognition rate of facial expressions utilizing existing and Efficientnet V2 approaches.\n",
    "\n",
    "•\tConsequently, the general goal is to research and generate effective deep learning approaches for automatic face recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Computer Vision and Face Emotions Recognition\n",
    "\n",
    "## Computer Vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a key component of Artificial Intelligence (AI) approaches, which tries to create intelligent algorithms that can perceive images as if they were seen by a person. It is a way of converting input data from an image into a decision result in which the transformation outcomes have a desire to achieve a goal.\n",
    "\n",
    "Computer vision is the ability of computers to comprehend digitally acquired images. Computer vision is an important technical solution for many complicated problems, but it is not without its challenges. It creates human-like vision abilities for numerous applications by integrating computer vision with DL systems. Computer vision now has an extensive variety of applications in the real world, including retail, finance, construction, sports, automobile, agriculture, insurance, and more. Some use cases include computer vision projects that have a positive impact on the world.\n",
    "\n",
    "One of the most important aspects of recognizing human emotions is the ability to recognize facial expressions. This field is part of the human-computer interaction study that benefits society. This field research could lead to the development of humanoid robots that can interact with humans and respond to human emotion, such as those used in child daycare. It can also be used in healthcare to detect patients' emotions in order to determine their mental health status. It can also be employed in the entertainment sector, for example, in video games that modify the game's flow based on the person's facial gestures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is critical to design advanced methods to address the aforementioned challenge in order to acquire the best solution. We are going to use Resnet VGG16 and Efficientnet_V2 for human facial expressions recognition to produce a valid solution in this project. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FER datasets are available online, however size of input image, colour, and image format, also labelling and directory arrangement, and differ greatly. To resolve these disparities, we simply divided the input datasets into seven catalogues (one for each class). During training, we loaded images from disc in batches (to avoid memory overflow) and used data generators to resize and format the images automatically.\n",
    "\n",
    "Because it comprises non-face shots, text images, drowsy faces, discernible profile photographs, and a large number of incorrectly labelled images, FER2013 suffers from severe crowdsourcing. As a result, utilizing this dataset, the overall accuracy of facial expression categorization is less than 65%. Data augmentation approaches were used to overcome this challenge by artificially creating training images using various processing methods such as random rotation, shifts, shear, and flips, among others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Agumentation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a set of approaches for artificially increasing the amount of data by creating more data points from current data. This can include making minor changes to data or using deep learning models to generate more data points. ImageDataGenerator enables it simple to create a real-time data augmentation image generator that generates batches of image data. The simplest basic code for creating and configuring ImageDataGenerator and training enhanced images on deep neural networks. It is a method of modifying existing data in order to generate new data for the model training process.\n",
    "\n",
    "To overcome this problem, data augmentation techniques were employed to create training images artificially utilising various processing methods such as random rotation, “shifts,“, “shear,“, then “,flips“,, among others. Data augmentation is a collection of approaches for insincerely cumulative the quantity of image/features data by creating more data-points after current features. This can include making minor changes to data or using deep learning models to generate more data points. ImageDataGenerator makes creating a better picture generator easier. ImageDataGenerator produces image data in batches and enhances it in real time. The most basic code for setting up ImageDataGenerator and training enhanced photos with deep neural networks. It's a technique for altering existing data in order to generate new data for model training. To put it another way, it's a method for artificially boosting the dataset available for deep learning model training. The following are the advantages of data augmentation methods: \n",
    "\n",
    "1. Improving the accuracy of model predictions.\n",
    "2. Size of Training data is increased in the DL models.\n",
    "3. Data over-fitting (a statistical error in which a function is too closely related to a limited group of data points) and    data variability are reduced.\n",
    "4. Improving the model's capacity to generalize.\n",
    "5. Assisting with concerns of categorization class imbalance.\n",
    "6. Lowering the costs of data collecting and labelling.\n",
    "7. Enables the prediction of uncommon events.\n",
    "\n",
    "Image transformations, such as rotation, flipping, and cropping, are among the most basic image manipulations. The majority of these approaches directly change photographs and are simple to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can horizontally and vertically flip images. Vertical flips are not supported by all frameworks. A vertical flip, on the other hand, is the same as rotating an image 180 degrees before flipping it horizontally. Here are some examples of photos that have been flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's vital to remember that following rotation, input image size/dimensions cannot be conserved. If your image is square, rotate it at right angles to maintain the same size. Turning it “180“degrees will maintain it the same size if it's a rectangle. As the image is rotated at finer angles, the final image size will change. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's conceivable to zoom in or out of the image. The final image size will be greater than the starting image size after scaling outward. Most image frameworks extract a chunk of the new image that is the same size as the old one. In the following session, we'll look into scaling inward, which reduces the image size and forces us to make assumptions about what is beyond the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise injection is the process of injecting a matrix of random values, often from a Gaussian distribution. Moreno-Barea et al. evaluated noise injection on nine datasets from the UCI collection. CNNs can learn more robust features by adding noise to pictures. Geometric adjustments can successfully address positional biases in training data. Training data distributions may differ from testing data distributions due to a variety of choices. If positional biases exist, such as in a facial recognition dataset where every face is properly centred in the image, geometric modifications are an excellent alternative. Geometric transformations are useful not only for eliminating positional biases, but also because they are simple to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is used to store the dimensions (height-width-color channels) of digital image data. Another useful method is to perform augmentations in the colour channels space. Isolating a single colour channel, such as R, G, or B, can be used for colour augmentation.By isolating one matrix and adding two zero matrices from the other colour channels, a picture can be swiftly transformed into its representation in one colour channel.\n",
    "\n",
    "Moreover, using basic matrix operations, the RGB values can be easily changed to enhance or lessen the image's brightness. More extensive colour augmentations can be achieved by creating a colour histogram that describes the image. The lighting effects produced by changing the intensity levels in these histograms are similar to those found in photo editing software.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast Stretching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a technique that examines the distribution of pixel densities in an image. Contrast is defined as the difference between light and dark hues in an image. Control the amount of jitter in contrast with the contrast parameter, which ranges from 0 (no change) to 1. (Potentially large change). Contrast does not define whether the augmented image's contrast will be higher or lower, only the effect's possible strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest edge is scaled to the specified size, while the longest edge is automatically adjusted to maintain the aspect ratio of the supplied image. It adjusts both height and width to the specified size, even if the aspect ratio is not preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike scaling, we just sample a piece of the original input image on unsystematic, which is then resized to the original format of image size. Random cropping is the name given to this technique. Here are some random cropping instances. The difference between this approach and scaling can be seen if you look attentively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sharpening input images/photos for preprocessing/Data Augmentation may consequence in additional facts about objects of area being captured. The traditional methods of applying kernel filters on photos include sharpening and blurring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During translation, the image is simply moved in the X or Y direction (or both). In the next example, we assume the image has a black background beyond its boundaries and translate it correctly. This form of augmentation is incredibly useful because most things can be found almost anywhere in the image. Your convolutional neural network is presently looking in every direction conceivable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of ToTensor and normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of converting the original values of picture pixels to a new set of values is known as image transformation. Converting a photo into a PyTorch tensor is one of the processes we conduct on photographs. The pixel values are scaled between 0.0 and 1.0 when converting an image to a PyTorch tensor. When using PyTorch, transforms have used to achieve this transition. ToTensor().\n",
    "\n",
    "Normalizing images is a valuable practise when working with deep neural networks. Normalizing images includes converting them to values such as 0.0 and 1.0 for the image's mean and standard deviation, respectively. Remove the network mean vale from each of the input channel of the system and divide the outcome by using the number of input channels to achieve this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FERR2013 is a collection of 48x48 facial photos with a variety of expressions. Faces in the Fer2013 dataset were automatically collected. As a result, the captured human face is centred then occupies the similer amount of size in each frame. The results of each emotion's Google image search, as well as alternatives for the feelings, were compiled into this dataset.\n",
    "\n",
    "Fer2013 datasets are used in a variety of applications that involve making decisions about facial expression detection tasks. Traditional methods, deep learning, pre-trained models, and ensemble neural networks techniques are used to handle these difficulties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, feature selection has a substantial impact on the performance of machine learning algorithms, potentially resulting in incorrect class separation. DL, unlike standard machine learning methods, can automate the learning of feature sets for a wide range of tasks. With DL, learning and classification may be done in one step. DL has been an exceedingly popular type of ML approach in recent years because to the massive expansion and innovation of the field of big data. Its novel performance for a number of machine learning tasks is still under development, but it has streamlined the improvement of so many AI learning domains for example input picture/ digital image super-resolution, main object discovery, and input image gratitude. On tasks like image categorization, DL performance has recently surpassed human performance.\n",
    "\n",
    "CNNs are a kind of DL that uses convolutional layers to filter meaningful information. The convolutional filters of the input image are used to calculate the yield of neurons associated to input image local area of interest to extract the spatial and temporal aspects of the image. In the convolutional layers of CNN, a weight sharing mechanism is used to lower the total number of input parameters. The CNN approach is often made up of three components:\n",
    "\n",
    "•\tUse a convolutional layer to learn spatial and temporal features.\n",
    "\n",
    "•\tA subsampling layer (max-pooling) to reduce or down sample the dimensionality of an input image.\n",
    "\n",
    "•\tA fully connected (FC) layer for categorizing the input image into different classifications.\n",
    "\n",
    "•\tConvolutional layer: Convolutional layers are created by applying a series of filters (also known as kernels) to an input image. The feature map created by using the convolutional layer is a representation of the input image with the filters applied. Convolutional layers can be stacked to build increasingly complicated models that can learn more complex features from photos.\n",
    "\n",
    "•\tPooling layer: In deep learning, pooling layers are a sort of convolutional layer. The spatial size of the input is reduced by pooling layers, making it easier to process and needing less memory. Pooling also minimises the amount of factors in the training process and speeds it up. Pooling can be divided into two types: maximal pooling and average pooling. The maximum value from each feature map is used in max pooling, while the average value is used in average pooling. After convolutional layers, pooling layers are typically used to minimise the amount of the input before it is delivered to a fully connected layer.\n",
    "\n",
    "•\tFully connected layer: These are one of the most fundamental types of layers of CNN method. Each neuron in this layer is entirely coupled to all other neuron of previous layer, as the name implies. This layer employed at the end of a CNN method when the goal is to use the features learned by the preceding layers to create predictions. If we were using a CNN to categorise photographs of animals, for example, the last fully connected layer could use the information learnt by the preceding layers to classify an image as including a dog, cat, bird, or other animal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG (Visual Geometry Group) -16                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The International ILSVRC is a \"computer vision,\" challenge held every year. Teams fight for two jobs each year. The first step is to locate objects within the image using the 200 classes, which is referred to as the object's local action. Teams fight for two jobs each year. The first step is to locate objects within the image using the 200 classes, which is referred to as the object's local action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaging partition is the next step, which entails sorting images into one of 1000 categories. In their paper \"VERY DEEP COVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,\" Karen Simonyan and Andrew Zisserman of Oxford University's Visual Geometry Group Lab suggested VGG 16 in 2014. In the 2014 ILSVRC event, this model took first and second place in the aforementioned categories.\n",
    "\n",
    "Following Steps 2 and 2, there is a large integration layer that is similar to the prior layer. The filter size (3, 3) and filter 256 are converted using two layers. There are two sets of three convolution layers after that, followed by a maximum pool layer. Each has the same number of filters and pads (3, 3).\n",
    "\n",
    "We employ size 3 * 3 filters instead of 11 * 11 on AlexNet and 7 * 7 on ZF-Net for these convolution layers and larger integration. It also changes the amount of input channels at different levels by using the 1 * 1 pixel. After each layer of rotation, a 1 pixel termination (parallel termination) is conducted to prevent the image's local element. We uncovered (7, 7, 512) a feature map after a lot of convolution and a max-pooling layer. We generate a component (1, 25088) vector by decrypting this output. There are three fully linked layers after that: the first subtracts the vector (1, 4096) from the input from the last element vector, the second extracts a vector size (1, 4096), and the third layer subtracts the vector (1, 4096) from the input after the last reduce the computational.\n",
    "\n",
    "Following the testing of the five vector segment classifications. The unlock function for all concealed layers is ReLU. ReLU is mathematically sound since it speeds up learning while lowering the possibility of gradient collapse. The VGG-16 network was trained using image databases of various sizes. The VGG-16 network provides good accuracies even when the picture data sets are small because to its thorough training. The VGG-16 network has 16 convolution layers and a 33 percent receptive field. There are a total of 5 of these levels, with a Max pooling layer of size 22. After the last Max pooling layer, there are three fully connected layers. The next three layers are all linked together. The softmax classifer is used as the final layer. All buried layers receive ReLu activation. The VGG-16 architecture is depicted schematically.\n",
    "\n",
    "The input image for VGG-19 in this work is 48x48 pixels in size. The input image is distributed over a series of convolution network layers, including 3x3 filters, 19 weight layers, 16 layers of convolution with 3x3 filter size, and 3 layers of completely connected, followed by a stack of convolutional layers. Each of the first two completely connected layers has 4096 channels, whereas the third uses a seven-way classification strategy and hence has seven channels (every class has a channel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following AlexNet's historic victory in the LSVRC2012 segmentation competition, the deep Residual Network has definitely been a key milestone in the computer / social learning process of in-depth learning during the previous few years. You can train hundreds or thousands of layers with ResNet and still achieve amazing results. Many computer vision applications that do not require image classification, such as object detection and face recognition, benefit from its powerful representation capabilities.\n",
    "\n",
    "Since it originally made news in 2015, many in the research community have looked into the secrets of ResNet's success, and several architectural enhancements have been devised. The first section of this essay will provide some background information for anyone unfamiliar with ResNet, and the second will address some recent publications I've read concerning the various types and classifications. If a specified volume is given, a single-layer feedforward network can represent any function, according to the universal approximation theorem. However, the layer could be quite large, and the network could suffer from data overload. As a result, there is widespread agreement among scientists that our network design should be improved. According to AlexNet, CNN's present design is becoming increasingly sophisticated. VGG and GoogleNet (also known as Inception v1) both have 19 and 22 layers to convert, respectively, although AlexNet just has five. In contrast, increasing the network's depth accomplishes more than just layering layers.\n",
    "\n",
    "The well-known gradient problem makes deep networks difficult to train: as the gradient is propagated back to previous layers, duplication can cause the gradient to decline endlessly. As a result, as the network goes deeper, its performance depletes or rapidly diminishes. There were a few approaches to dealing with gradient loss before ResNet, such as adding auxiliary losses to the middle layer as additional monitoring, but none of them seemed to solve the problem permanently. According to the authors, packaging layers should have no impact on network performance, according to the authors, because we can just accumulate identifiable mappings (the blank layer) in the existing network, and the final structures will do the same. As a result, a DL would not yield more training error rate than shallow models. \n",
    "\n",
    "It believe that matching loaded model layers to the residual map is informal than permitting the exact layout map you want. And the last block indicated earlier clearly allows it to do so. In reality, Highway Network was the first to use gateway shortcut connections, not ResNet. These settings are controlled by the quantity of data that can be sent via the shortcut. The amount of information that travels to the next stage of the Short-Term Memory Cell (LSTM), which is based on the same idea, is determined by a parameter forgetting gate. As a result, ResNet might be considered a one-of-a-kind Highway Network story. ResNet50 is divided into two types: ResNet50 and SEResnet50. ResNet50 is a shortened version of a 50-layer residual network. Resnet50 is comparable. Resnet50 is similar to VGG-16 with the exception that it adds a supplementary individuality plotting ability. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFFCIENTNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A revolutionary family of deep convolutional having quicker training speed and better parameter optimization than past versions, is presented in this work. We employ a hybrid of 'training-aware' neural network learning models for search as well as scale for enhancing learning rate and parameter optimization in these networks. The models were designed in a search space that were extended to include new events like Fused-MBConv. EfficientNetV2 networks train up to 6.8 times quicker than province models, as per our experiments. Researchers can speed up the training by steadily increasing the size of the image during it, although often this results in the loss of accuracy. To cover the loss of accuracy, we propose an improved progressive learning method which balances training data (e.g. data augmentation) and input image pixel density.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the knowledge of a model that has collected training for a particular task to solve a different but related task. The model can benefit from the lessons learned from the previous work so that the new one learns quickly. Let's make an example here and say you want to see the dogs in the pictures. On the internet you find a model that can see cats. Since this is the same job enough, you take a few pictures of your dog and retrain the model from seeing the dogs.\n",
    "\n",
    "There are actually two types of learning transfer, feature removal and fine adjustment.\n",
    "Both methods usually follow the same procedure:\n",
    "Launch a pre-trained model (the model we want to learn from)\n",
    "Rearrange the final layers to have the same output value as the category included in the new database\n",
    "Explain what layers we want to review\n",
    "Train to new database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LConsider the creation of a convolutional neural network with a dense layer and a single output neuron as filters. The network has been taught to predict the presence of a cat in an image. We'll need a large data set (photos with and without cats), and the training process will take a lengthy time. This is referred to as \"pre-training.\"\n",
    "\n",
    "Then there's the enjoyable part. We retrain the network with a data set of a small image with dogs this time. All layers except the exit layer get \"snow\" during training. This means that throughout training, we do not renew. Following training, the network eliminates the possibility of the dog appearing in the image. This training will be quicker than the initial training. We can also \"release\" the last two levels, the output and the compact layer, voluntarily. This is dependent on how much data we have. If we just have a tiny amount of data, we may only train the final layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning example in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work differs from that of my previous example. A model to see which dogs and cats are in which photographs. For the code to work you will need to organize your data in the following format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: First, we need to install the required libraries for building the network. \n",
    "\n",
    "Step2: Import the required machine learning libraries and pytorch frame work to evaluate the model efficiency.\n",
    "\n",
    "\n",
    "Step3: Identifies the number of processor/thread available in the system.\n",
    "\n",
    "Step4: Load the data from the folder and preprocess and augment the data based on the train/test/val. Batch size of the network can be decided based on the system GPU/CPU cache memory for our consideration batch size as 32. Based on the set name parameter the preprocessing and augmentation might be apply. After read, preprocess and augment the image we can shuffle the dataset by setting the parameter true.\n",
    "\n",
    "We then create different lists of storing the testing and training image pixels. After this, we check if the pixel belongs to training then we append it into the training list & training labels. Similarly, for pixels belonging to the Public test, we append it to testing lists. After doing this we convert the training labels and testing labels into categorical ones. The code is given below.\n",
    "\n",
    "Step5: Predict the trained model based on the input data and calculate the loss of the model using the criterion parameter.\n",
    "\n",
    "Step6: Initialize the required parameter. Image size should be 48 x 48. Number of different class in face emotion detection consideration is 7. Dataset folder needs to be initialized in data_dir. Changing the trained_model_weight path can give you the different model accuracy results currently we provide the efficiennet v2 small model weights.\n",
    "\n",
    "Step7: Import the dataset image into torch object\n",
    "\n",
    "Step8: Import the effcientnet version2 small model base architecture and load the pretrained weights for the same model.\n",
    "\n",
    "Step9: Import the VGG16 model base architecture and load the pretrained weights for the same model.\n",
    "\n",
    "Step10: Import the RESNET model base architecture and load the pretrained weights for the same model.\n",
    "\n",
    "Step11: Import the Efficientnet V2 model base architecture and load the pretrained weights for the same model. \n",
    "\n",
    "Step12: Compare the results \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result and Discussion\n",
    "\n",
    "All of our studies were run on a 64-bit Windows 10 OS with an Intel Core i5-7200U (7th Gen) CPU. The results achieved utilising the three pre-trained networks, viz. VGG-16, Resnet50, and efficientnetv2, as classifiers to categorise an image into the seven different states, viz. Anger, Contempt, Disgust, Fear, Happy, Sad, and Surprise, are discussed in this part. We compared the findings that they produced. Photos from the training set are used to train the networks, while images from the validation set are used to test them. The validation set consists of photos that have never been viewed previously by the network. To improve the network's training, the data was shuffled.\n",
    "\n",
    "The method was tested on Google Colab with GPU acceleration enabled using PyTorch. Because of the magnitude of the training data and the lifespan of Google Colab, the researchers had to implement a run-and-pause mechanism every 10 epochs as part of the training mechanism to avoid an abrupt disengagement from the machine, which would cause the training process to end prematurely. The training and validation were completed in a single period. The test was repeated on the model every 10 epochs after each run-and-pause, and the results were recorded.\n",
    "\n",
    "Many new models have emerged as a result of the application of deep learning for computer vision problems, which outperform previous models. Deep learning faces a significant problem in image categorization and recognition. And it has advanced significantly in recent years. The new EfficientNet models have appeared as a result of this. In image categorization, these models outperformed several previous deep learning models. We now have access to all of the EfficientNet models due to the recent release of PyTorch 1.10. We will use the PyTorch pretrained EfficientNet model to do picture classification in this project.\n",
    "\n",
    "# PyTorch Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have access to the pre-trained EfficientNet models with PyTorch version 1.10. Using PyTorch's torchvision module, we can access the models. In reality, PyTorch includes all models trained on the ImageNet dataset, from EfficientNetB0 to EfficientNetB7. This means that if our requirements are similar to those of the pretrained models, we may immediately load and use these models for image classification jobs. We can also apply these to our own dataset for transfer learning and fine-tuning. We'll use EfficientNetB0 to do picture classification and compare the CPU and GPU forward pass times to ResNet and VGG16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intention of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The JAFFE, Cohn-Kanade Dataset (CK), Extended CohnKanade Dataset, KDEF, AffectNet, Static Facial Expression in the Wild (SFEW), and FER2013 are all popular datasets for facial expression recognition research. FER2013 was first presented at the International Conference on Machine Learning (ICML) and is now widely utilised in facial emotion recognition research. Based on a study of prior facial expression recognition research using the FER2013 dataset, we discovered that using ensembled CNN, the maximum accuracy was 76.82 percent, whereas research using a single model without additional data achieved 73.28 percent accuracy. Based on this study, the difficulty of improving the accuracy performance of the model, particularly on a single model, remains. Many experiments with FER2013 use a variety of models, and one prominent architecture is VGG, which has reached state of the art a few times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required machine learning library library and pytorch frame work to evaluate the model efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\exterro\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import os\n",
    "from os import makedirs\n",
    "from os.path import join, isfile, isdir\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "print(module_path)\n",
    "import sys\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "# from efficientnet_v2 import EfficientNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from math import ceil, floor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections.abc as container_abc\n",
    "from torch.utils import model_zoo\n",
    "\n",
    "\n",
    "def _pair(x):\n",
    "    if isinstance(x, container_abc.Iterable):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "\n",
    "def torch_conv_out_spatial_shape(in_spatial_shape, kernel_size, stride):\n",
    "    if in_spatial_shape is None:\n",
    "        return None\n",
    "    # in_spatial_shape -> [H,W]\n",
    "    hin, win = _pair(in_spatial_shape)\n",
    "    kh, kw = _pair(kernel_size)\n",
    "    sh, sw = _pair(stride)\n",
    "\n",
    "    # dilation and padding are ignored since they are always fixed in efficientnetV2\n",
    "    hout = int(floor((hin - kh - 1) / sh + 1))\n",
    "    wout = int(floor((win - kw - 1) / sw + 1))\n",
    "    return hout, wout\n",
    "\n",
    "\n",
    "def get_activation(act_fn: str, **kwargs):\n",
    "    if act_fn in ('silu', 'swish'):\n",
    "        return nn.SiLU(**kwargs)\n",
    "    elif act_fn == 'relu':\n",
    "        return nn.ReLU(**kwargs)\n",
    "    elif act_fn == 'relu6':\n",
    "        return nn.ReLU6(**kwargs)\n",
    "    elif act_fn == 'elu':\n",
    "        return nn.ELU(**kwargs)\n",
    "    elif act_fn == 'leaky_relu':\n",
    "        return nn.LeakyReLU(**kwargs)\n",
    "    elif act_fn == 'selu':\n",
    "        return nn.SELU(**kwargs)\n",
    "    elif act_fn == 'mish':\n",
    "        return nn.Mish(**kwargs)\n",
    "    else:\n",
    "        raise ValueError('Unsupported act_fn {}'.format(act_fn))\n",
    "\n",
    "\n",
    "def round_filters(filters, width_coefficient, depth_divisor=8):\n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    min_depth = depth_divisor\n",
    "\n",
    "    filters *= width_coefficient\n",
    "    new_filters = max(min_depth, int(\n",
    "        filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += depth_divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, depth_coefficient):\n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    return int(ceil(depth_coefficient * repeats))\n",
    "\n",
    "\n",
    "class DropConnect(nn.Module):\n",
    "    def __init__(self, rate=0.5):\n",
    "        super(DropConnect, self).__init__()\n",
    "        self.keep_prob = None\n",
    "        self.set_rate(rate)\n",
    "\n",
    "    def set_rate(self, rate):\n",
    "        if not 0 <= rate < 1:\n",
    "            raise ValueError(\n",
    "                \"rate must be 0<=rate<1, got {} instead\".format(rate))\n",
    "        self.keep_prob = 1 - rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            random_tensor = self.keep_prob + torch.rand([x.size(0), 1, 1, 1],\n",
    "                                                        dtype=x.dtype,\n",
    "                                                        device=x.device)\n",
    "            binary_tensor = torch.floor(random_tensor)\n",
    "            return torch.mul(torch.div(x, self.keep_prob), binary_tensor)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class SamePaddingConv2d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_spatial_shape,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 dilation=1,\n",
    "                 enforce_in_spatial_shape=False,\n",
    "                 **kwargs):\n",
    "        super(SamePaddingConv2d, self).__init__()\n",
    "\n",
    "        self._in_spatial_shape = _pair(in_spatial_shape)\n",
    "        # e.g. throw exception if input spatial shape does not match in_spatial_shape\n",
    "        # when calling self.forward()\n",
    "        self.enforce_in_spatial_shape = enforce_in_spatial_shape\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        dilation = _pair(dilation)\n",
    "\n",
    "        in_height, in_width = self._in_spatial_shape\n",
    "        filter_height, filter_width = kernel_size\n",
    "        stride_heigth, stride_width = stride\n",
    "        dilation_height, dilation_width = dilation\n",
    "\n",
    "        out_height = int(ceil(float(in_height) / float(stride_heigth)))\n",
    "        out_width = int(ceil(float(in_width) / float(stride_width)))\n",
    "\n",
    "        pad_along_height = max((out_height - 1) * stride_heigth +\n",
    "                               filter_height + (filter_height - 1) * (dilation_height - 1) - in_height, 0)\n",
    "        pad_along_width = max((out_width - 1) * stride_width +\n",
    "                              filter_width + (filter_width - 1) * (dilation_width - 1) - in_width, 0)\n",
    "\n",
    "        pad_top = pad_along_height // 2\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_left = pad_along_width // 2\n",
    "        pad_right = pad_along_width - pad_left\n",
    "\n",
    "        paddings = (pad_left, pad_right, pad_top, pad_bottom)\n",
    "        if any(p > 0 for p in paddings):\n",
    "            self.zero_pad = nn.ZeroPad2d(paddings)\n",
    "        else:\n",
    "            self.zero_pad = None\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=stride,\n",
    "                              dilation=dilation,\n",
    "                              **kwargs)\n",
    "\n",
    "        self._out_spatial_shape = (out_height, out_width)\n",
    "\n",
    "    @property\n",
    "    def out_spatial_shape(self):\n",
    "        return self._out_spatial_shape\n",
    "\n",
    "    def check_spatial_shape(self, x):\n",
    "        if x.size(2) != self._in_spatial_shape[0] or \\\n",
    "                x.size(3) != self._in_spatial_shape[1]:\n",
    "            raise ValueError(\n",
    "                \"Expected input spatial shape {}, got {} instead\".format(self._in_spatial_shape,\n",
    "                                                                         x.shape[2:]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.enforce_in_spatial_shape:\n",
    "            self.check_spatial_shape(x)\n",
    "        if self.zero_pad is not None:\n",
    "            x = self.zero_pad(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SqueezeExcitate(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 se_size,\n",
    "                 activation=None):\n",
    "        super(SqueezeExcitate, self).__init__()\n",
    "        self.dim_reduce = nn.Conv2d(in_channels=in_channels,\n",
    "                                    out_channels=se_size,\n",
    "                                    kernel_size=1)\n",
    "        self.dim_restore = nn.Conv2d(in_channels=se_size,\n",
    "                                     out_channels=in_channels,\n",
    "                                     kernel_size=1)\n",
    "        self.activation = F.relu if activation is None else activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = self.dim_reduce(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dim_restore(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return torch.mul(inp, x)\n",
    "\n",
    "\n",
    "class MBConvBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 expansion_factor,\n",
    "                 act_fn,\n",
    "                 act_kwargs={},\n",
    "                 bn_epsilon=None,\n",
    "                 bn_momentum=None,\n",
    "                 se_size=None,\n",
    "                 drop_connect_rate=None,\n",
    "                 bias=False,\n",
    "                 tf_style_conv=False,\n",
    "                 in_spatial_shape=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        exp_channels = in_channels * expansion_factor\n",
    "\n",
    "        self.ops_lst = []\n",
    "\n",
    "        # expansion convolution\n",
    "        if expansion_factor != 1:\n",
    "            self.expand_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=exp_channels,\n",
    "                kernel_size=1,\n",
    "                bias=bias)\n",
    "\n",
    "            self.expand_bn = nn.BatchNorm2d(\n",
    "                num_features=exp_channels,\n",
    "                eps=bn_epsilon,\n",
    "                momentum=bn_momentum)\n",
    "\n",
    "            self.expand_act = get_activation(act_fn, **act_kwargs)\n",
    "            self.ops_lst.extend(\n",
    "                [self.expand_conv, self.expand_bn, self.expand_act])\n",
    "\n",
    "        # depth-wise convolution\n",
    "        if tf_style_conv:\n",
    "            self.dp_conv = SamePaddingConv2d(\n",
    "                in_spatial_shape=in_spatial_shape,\n",
    "                in_channels=exp_channels,\n",
    "                out_channels=exp_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                groups=exp_channels,\n",
    "                bias=bias)\n",
    "            self.out_spatial_shape = self.dp_conv.out_spatial_shape\n",
    "        else:\n",
    "            self.dp_conv = nn.Conv2d(\n",
    "                in_channels=exp_channels,\n",
    "                out_channels=exp_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "                groups=exp_channels,\n",
    "                bias=bias)\n",
    "            self.out_spatial_shape = torch_conv_out_spatial_shape(\n",
    "                in_spatial_shape, kernel_size, stride)\n",
    "\n",
    "        self.dp_bn = nn.BatchNorm2d(\n",
    "            num_features=exp_channels,\n",
    "            eps=bn_epsilon,\n",
    "            momentum=bn_momentum)\n",
    "\n",
    "        self.dp_act = get_activation(act_fn, **act_kwargs)\n",
    "        self.ops_lst.extend(\n",
    "            [self.dp_conv, self.dp_bn, self.dp_act])\n",
    "\n",
    "        # Squeeze and Excitate\n",
    "        if se_size is not None:\n",
    "            self.se = SqueezeExcitate(exp_channels,\n",
    "                                      se_size,\n",
    "                                      activation=get_activation(act_fn, **act_kwargs))\n",
    "            self.ops_lst.append(self.se)\n",
    "\n",
    "        # projection layer\n",
    "        self.project_conv = nn.Conv2d(\n",
    "            in_channels=exp_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=bias)\n",
    "\n",
    "        self.project_bn = nn.BatchNorm2d(\n",
    "            num_features=out_channels,\n",
    "            eps=bn_epsilon,\n",
    "            momentum=bn_momentum)\n",
    "\n",
    "        # no activation function in projection layer\n",
    "\n",
    "        self.ops_lst.extend(\n",
    "            [self.project_conv, self.project_bn])\n",
    "\n",
    "        self.skip_enabled = in_channels == out_channels and stride == 1\n",
    "\n",
    "        if self.skip_enabled and drop_connect_rate is not None:\n",
    "            self.drop_connect = DropConnect(drop_connect_rate)\n",
    "            self.ops_lst.append(self.drop_connect)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        for op in self.ops_lst:\n",
    "            x = op(x)\n",
    "        if self.skip_enabled:\n",
    "            return x + inp\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class FusedMBConvBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 expansion_factor,\n",
    "                 act_fn,\n",
    "                 act_kwargs={},\n",
    "                 bn_epsilon=None,\n",
    "                 bn_momentum=None,\n",
    "                 se_size=None,\n",
    "                 drop_connect_rate=None,\n",
    "                 bias=False,\n",
    "                 tf_style_conv=False,\n",
    "                 in_spatial_shape=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        exp_channels = in_channels * expansion_factor\n",
    "\n",
    "        self.ops_lst = []\n",
    "\n",
    "        # expansion convolution\n",
    "        if expansion_factor != 1:\n",
    "            if tf_style_conv:\n",
    "                self.expand_conv = SamePaddingConv2d(\n",
    "                    in_spatial_shape=in_spatial_shape,\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=exp_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    bias=bias)\n",
    "            else:\n",
    "                self.expand_conv = nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=exp_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=1,\n",
    "                    stride=stride,\n",
    "                    bias=bias)\n",
    "\n",
    "            self.expand_bn = nn.BatchNorm2d(\n",
    "                num_features=exp_channels,\n",
    "                eps=bn_epsilon,\n",
    "                momentum=bn_momentum)\n",
    "\n",
    "            self.expand_act = get_activation(act_fn, **act_kwargs)\n",
    "            self.ops_lst.extend(\n",
    "                [self.expand_conv, self.expand_bn, self.expand_act])\n",
    "\n",
    "        # Squeeze and Excitate\n",
    "        if se_size is not None:\n",
    "            self.se = SqueezeExcitate(exp_channels,\n",
    "                                      se_size,\n",
    "                                      activation=get_activation(act_fn, **act_kwargs))\n",
    "            self.ops_lst.append(self.se)\n",
    "\n",
    "        # projection layer\n",
    "        kernel_size = 1 if expansion_factor != 1 else kernel_size\n",
    "        stride = 1 if expansion_factor != 1 else stride\n",
    "        if tf_style_conv:\n",
    "            self.project_conv = SamePaddingConv2d(\n",
    "                in_spatial_shape=in_spatial_shape,\n",
    "                in_channels=exp_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                bias=bias)\n",
    "            self.out_spatial_shape = self.project_conv.out_spatial_shape\n",
    "        else:\n",
    "            self.project_conv = nn.Conv2d(\n",
    "                in_channels=exp_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=1 if kernel_size > 1 else 0,\n",
    "                bias=bias)\n",
    "            self.out_spatial_shape = torch_conv_out_spatial_shape(\n",
    "                in_spatial_shape, kernel_size, stride)\n",
    "\n",
    "        self.project_bn = nn.BatchNorm2d(\n",
    "            num_features=out_channels,\n",
    "            eps=bn_epsilon,\n",
    "            momentum=bn_momentum)\n",
    "\n",
    "        self.ops_lst.extend(\n",
    "            [self.project_conv, self.project_bn])\n",
    "\n",
    "        if expansion_factor == 1:\n",
    "            self.project_act = get_activation(act_fn, **act_kwargs)\n",
    "            self.ops_lst.append(self.project_act)\n",
    "\n",
    "        self.skip_enabled = in_channels == out_channels and stride == 1\n",
    "\n",
    "        if self.skip_enabled and drop_connect_rate is not None:\n",
    "            self.drop_connect = DropConnect(drop_connect_rate)\n",
    "            self.ops_lst.append(self.drop_connect)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        for op in self.ops_lst:\n",
    "            x = op(x)\n",
    "        if self.skip_enabled:\n",
    "            return x + inp\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class EfficientNetV2(nn.Module):\n",
    "    _models = {'s': {'num_repeat': [2, 4, 4, 6, 9, 15],\n",
    "                     'kernel_size': [3, 3, 3, 3, 3, 3],\n",
    "                     'stride': [1, 2, 2, 2, 1, 2],\n",
    "                     'expand_ratio': [1, 4, 4, 4, 6, 6],\n",
    "                     'in_channel': [24, 24, 48, 64, 128, 160],\n",
    "                     'out_channel': [24, 48, 64, 128, 160, 256],\n",
    "                     'se_ratio': [None, None, None, 0.25, 0.25, 0.25],\n",
    "                     'conv_type': [1, 1, 1, 0, 0, 0],\n",
    "                     'is_feature_stage': [False, True, True, False, True, True],\n",
    "                     'width_coefficient': 1.0,\n",
    "                     'depth_coefficient': 1.0,\n",
    "                     'train_size': 300,\n",
    "                     'eval_size': 384,\n",
    "                     'dropout': 0.2,\n",
    "                     'weight_url': 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBdGlRcHc5VGNjZmllbFF5VWJOZzd0cmhBbm8/root/content',\n",
    "                     'model_name': 'efficientnet_v2_s_21k_ft1k-dbb43f38.pth'},\n",
    "               'm': {'num_repeat': [3, 5, 5, 7, 14, 18, 5],\n",
    "                     'kernel_size': [3, 3, 3, 3, 3, 3, 3],\n",
    "                     'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "                     'expand_ratio': [1, 4, 4, 4, 6, 6, 6],\n",
    "                     'in_channel': [24, 24, 48, 80, 160, 176, 304],\n",
    "                     'out_channel': [24, 48, 80, 160, 176, 304, 512],\n",
    "                     'se_ratio': [None, None, None, 0.25, 0.25, 0.25, 0.25],\n",
    "                     'conv_type': [1, 1, 1, 0, 0, 0, 0],\n",
    "                     'is_feature_stage': [False, True, True, False, True, False, True],\n",
    "                     'width_coefficient': 1.0,\n",
    "                     'depth_coefficient': 1.0,\n",
    "                     'train_size': 384,\n",
    "                     'eval_size': 480,\n",
    "                     'dropout': 0.3,\n",
    "                     'weight_url': 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBdGlRcHc5VGNjZmllN1ZDazRFb0o1bnlyNUE/root/content',\n",
    "                     'model_name': 'efficientnet_v2_m_21k_ft1k-da8e56c0.pth'},\n",
    "               'l': {'num_repeat': [4, 7, 7, 10, 19, 25, 7],\n",
    "                     'kernel_size': [3, 3, 3, 3, 3, 3, 3],\n",
    "                     'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "                     'expand_ratio': [1, 4, 4, 4, 6, 6, 6],\n",
    "                     'in_channel': [32, 32, 64, 96, 192, 224, 384],\n",
    "                     'out_channel': [32, 64, 96, 192, 224, 384, 640],\n",
    "                     'se_ratio': [None, None, None, 0.25, 0.25, 0.25, 0.25],\n",
    "                     'conv_type': [1, 1, 1, 0, 0, 0, 0],\n",
    "                     'is_feature_stage': [False, True, True, False, True, False, True],\n",
    "                     'feature_stages': [1, 2, 4, 6],\n",
    "                     'width_coefficient': 1.0,\n",
    "                     'depth_coefficient': 1.0,\n",
    "                     'train_size': 384,\n",
    "                     'eval_size': 480,\n",
    "                     'dropout': 0.4,\n",
    "                     'weight_url': 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBdGlRcHc5VGNjZmlmcmIyRHEtQTBhUTBhWVE/root/content',\n",
    "                     'model_name': 'efficientnet_v2_l_21k_ft1k-08121eee.pth'},\n",
    "               'xl': {'num_repeat': [4, 8, 8, 16, 24, 32, 8],\n",
    "                      'kernel_size': [3, 3, 3, 3, 3, 3, 3],\n",
    "                      'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "                      'expand_ratio': [1, 4, 4, 4, 6, 6, 6],\n",
    "                      'in_channel': [32, 32, 64, 96, 192, 256, 512],\n",
    "                      'out_channel': [32, 64, 96, 192, 256, 512, 640],\n",
    "                      'se_ratio': [None, None, None, 0.25, 0.25, 0.25, 0.25],\n",
    "                      'conv_type': [1, 1, 1, 0, 0, 0, 0],\n",
    "                      'is_feature_stage': [False, True, True, False, True, False, True],\n",
    "                      'feature_stages': [1, 2, 4, 6],\n",
    "                      'width_coefficient': 1.0,\n",
    "                      'depth_coefficient': 1.0,\n",
    "                      'train_size': 384,\n",
    "                      'eval_size': 512,\n",
    "                      'dropout': 0.4,\n",
    "                      'weight_url': 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBdGlRcHc5VGNjZmlmVXQtRHJLa21taUkxWkE/root/content',\n",
    "                      'model_name': 'efficientnet_v2_xl_21k_ft1k-1fcc9744.pth'}}\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 in_channels=3,\n",
    "                 n_classes=1000,\n",
    "                 tf_style_conv=False,\n",
    "                 in_spatial_shape=None,\n",
    "                 activation='silu',\n",
    "                 activation_kwargs=None,\n",
    "                 bias=False,\n",
    "                 drop_connect_rate=0.2,\n",
    "                 dropout_rate=None,\n",
    "                 bn_epsilon=1e-3,\n",
    "                 bn_momentum=0.01,\n",
    "                 pretrained=False,\n",
    "                 progress=False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.model_name = model_name\n",
    "        self.cfg = self._models[model_name]\n",
    "\n",
    "        if tf_style_conv and in_spatial_shape is None:\n",
    "            in_spatial_shape = self.cfg['eval_size']\n",
    "\n",
    "        activation_kwargs = {} if activation_kwargs is None else activation_kwargs\n",
    "        dropout_rate = self.cfg['dropout'] if dropout_rate is None else dropout_rate\n",
    "        _input_ch = in_channels\n",
    "\n",
    "        self.feature_block_ids = []\n",
    "\n",
    "        # stem\n",
    "        if tf_style_conv:\n",
    "            self.stem_conv = SamePaddingConv2d(\n",
    "                in_spatial_shape=in_spatial_shape,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=round_filters(\n",
    "                    self.cfg['in_channel'][0], self.cfg['width_coefficient']),\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                bias=bias\n",
    "            )\n",
    "            in_spatial_shape = self.stem_conv.out_spatial_shape\n",
    "        else:\n",
    "            self.stem_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=round_filters(\n",
    "                    self.cfg['in_channel'][0], self.cfg['width_coefficient']),\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=bias\n",
    "            )\n",
    "\n",
    "        self.stem_bn = nn.BatchNorm2d(\n",
    "            num_features=round_filters(\n",
    "                self.cfg['in_channel'][0], self.cfg['width_coefficient']),\n",
    "            eps=bn_epsilon,\n",
    "            momentum=bn_momentum)\n",
    "\n",
    "        self.stem_act = get_activation(activation, **activation_kwargs)\n",
    "\n",
    "        drop_connect_rates = self.get_dropconnect_rates(drop_connect_rate)\n",
    "\n",
    "        stages = zip(*[self.cfg[x] for x in ['num_repeat', 'kernel_size', 'stride',\n",
    "                                             'expand_ratio', 'in_channel', 'out_channel', 'se_ratio', 'conv_type', 'is_feature_stage']])\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        for stage_args in stages:\n",
    "            (num_repeat, kernel_size, stride, expand_ratio,\n",
    "             in_channels, out_channels, se_ratio, conv_type, is_feature_stage) = stage_args\n",
    "\n",
    "            in_channels = round_filters(\n",
    "                in_channels, self.cfg['width_coefficient'])\n",
    "            out_channels = round_filters(\n",
    "                out_channels, self.cfg['width_coefficient'])\n",
    "            num_repeat = round_repeats(\n",
    "                num_repeat, self.cfg['depth_coefficient'])\n",
    "\n",
    "            conv_block = MBConvBlockV2 if conv_type == 0 else FusedMBConvBlockV2\n",
    "\n",
    "            for _ in range(num_repeat):\n",
    "                se_size = None if se_ratio is None else max(\n",
    "                    1, int(in_channels * se_ratio))\n",
    "                _b = conv_block(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    expansion_factor=expand_ratio,\n",
    "                    act_fn=activation,\n",
    "                    act_kwargs=activation_kwargs,\n",
    "                    bn_epsilon=bn_epsilon,\n",
    "                    bn_momentum=bn_momentum,\n",
    "                    se_size=se_size,\n",
    "                    drop_connect_rate=drop_connect_rates[idx],\n",
    "                    bias=bias,\n",
    "                    tf_style_conv=tf_style_conv,\n",
    "                    in_spatial_shape=in_spatial_shape\n",
    "                )\n",
    "                self.blocks.append(_b)\n",
    "                idx += 1\n",
    "                if tf_style_conv:\n",
    "                    in_spatial_shape = _b.out_spatial_shape\n",
    "                in_channels = out_channels\n",
    "                stride = 1\n",
    "\n",
    "            if is_feature_stage:\n",
    "                self.feature_block_ids.append(idx - 1)\n",
    "\n",
    "        head_conv_out_channels = round_filters(\n",
    "            1280, self.cfg['depth_coefficient'])\n",
    "\n",
    "        self.head_conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=head_conv_out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=bias)\n",
    "        self.head_bn = nn.BatchNorm2d(\n",
    "            num_features=head_conv_out_channels,\n",
    "            eps=bn_epsilon,\n",
    "            momentum=bn_momentum)\n",
    "        self.head_act = get_activation(activation, **activation_kwargs)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.avpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(head_conv_out_channels, n_classes)\n",
    "\n",
    "        if pretrained:\n",
    "            self._load_state(_input_ch, n_classes, progress, tf_style_conv)\n",
    "\n",
    "        return\n",
    "\n",
    "    def _load_state(self, in_channels, n_classes, progress, tf_style_conv):\n",
    "        state_dict = model_zoo.load_url(\n",
    "            self.cfg['weight_url'],\n",
    "            progress=progress,\n",
    "            file_name=self.cfg['model_name'])\n",
    "\n",
    "        strict = True\n",
    "\n",
    "        if not tf_style_conv:\n",
    "            state_dict = OrderedDict([(k.replace('.conv.', '.'), v) if '.conv.' in k else (\n",
    "                k, v) for k, v in state_dict.items()])\n",
    "\n",
    "        if in_channels != 3:\n",
    "            if tf_style_conv:\n",
    "                state_dict.pop('stem_conv.conv.weight')\n",
    "            else:\n",
    "                state_dict.pop('stem_conv.weight')\n",
    "            strict = False\n",
    "\n",
    "        if n_classes != 1000:\n",
    "            state_dict.pop('fc.weight')\n",
    "            state_dict.pop('fc.bias')\n",
    "            strict = False\n",
    "\n",
    "        self.load_state_dict(state_dict, strict=strict)\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "\n",
    "    def get_dropconnect_rates(self, drop_connect_rate):\n",
    "        nr = self.cfg['num_repeat']\n",
    "        dc = self.cfg['depth_coefficient']\n",
    "        total = sum(round_repeats(nr[i], dc) for i in range(len(nr)))\n",
    "        return [drop_connect_rate * i / total for i in range(total)]\n",
    "\n",
    "    def get_features(self, x):\n",
    "        x = self.stem_act(self.stem_bn(self.stem_conv(x)))\n",
    "\n",
    "        features = []\n",
    "        feat_idx = 0\n",
    "        for block_idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if block_idx == self.feature_block_ids[feat_idx]:\n",
    "                features.append(x)\n",
    "                feat_idx += 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_act(self.stem_bn(self.stem_conv(x)))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.head_act(self.head_bn(self.head_conv(x)))\n",
    "\n",
    "        x = self.dropout(torch.flatten(self.avpool(x), 1))\n",
    "\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indentifies the number of processor/thread available in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the folder and preprocess and augment the data based on the train/test/val.\n",
    "Batch size of the network can be decided based on the sytem GPU/CPU chache memory for our consideration batch size as 32.\n",
    "Based on the set name parameter the preprocessing and agumentation might be apply.\n",
    "After read, preprocess and augment the image we can shuffle the dataset by setting the parameter true.\n",
    "\n",
    "We then create different lists of storing the testing and training image pixels. After this, we check if the pixel belongs to training then we append it into the training list & training labels. Similarly, for pixels belonging to the Public test, we append it to testing lists. After doing this we convert the training labels and testing labels into categorical ones. The code  is given below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(data_dir, batch_size, set_name, shuffle):\n",
    "    input_size = 48\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=0.5,std=0.5),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "\t'val': transforms.Compose([\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "    image_datasets = {x: datasets.ImageFolder(data_dir, data_transforms[x]) for x in [set_name]}\n",
    "      \n",
    "    dataset_loaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=shuffle, num_workers=num_workers) for x in [set_name]}\n",
    "    data_set_sizes = len(image_datasets[set_name])\n",
    "    return dataset_loaders, data_set_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the trained model based on the input data and calculate the loss of the model using the criterion parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, device, criterion=None):\n",
    "    loss_value = []\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            if out.size(1) == 1:\n",
    "                # regression, squeeze output of shape [N,1] to [N]\n",
    "                out = torch.squeeze(out, 1)\n",
    "            if criterion is not None:\n",
    "                loss = criterion(out, yb)\n",
    "                loss_value.append(loss.item())\n",
    "            y_pred.append(out.detach().cpu())\n",
    "            y_true.append(yb.detach().cpu())\n",
    "    if criterion is not None:\n",
    "        loss_value = sum(loss_value) / len(loss_value)\n",
    "        return torch.cat(y_pred), torch.cat(y_true), loss_value\n",
    "    else:\n",
    "        return torch.cat(y_pred), torch.cat(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and display the accuracy and F1 score measure of each class in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(y_pred, y_true):\n",
    "    _, y_pred = torch.max(y_pred, 1)\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    return accuracy_score(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intiatize the required parameter.\n",
    "Image size should be 48 x 48.\n",
    "number of different class in face emotion detection considereation is 7.\n",
    "Dataset folder needs to be initialized in data_dir.\n",
    "Changing the trained_model_weight path can give you the different model accuracy results \n",
    "Currently we provide the efficiennet v2 small model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_index = 's'\n",
    "img_size = 48\n",
    "n_classes = 7\n",
    "pretrained = False\n",
    "data_dir = r\"./Dataset/FER/\"\n",
    "traind_effv2_model_weight = \"./input/best_weights_effv2_s_Adam.pth\"\n",
    "traind_resnet_model_weight = \"./input/best_weights_resnet_Adam.pth\"\n",
    "traind_vgg_model_weight = \"./input/best_weights_VGG16_Adam.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset image into torch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  data size :  7066\n"
     ]
    }
   ],
   "source": [
    "dataloader_val_loader, validation_sizes = loaddata(data_dir=os.path.join(data_dir, \"test\"), batch_size=batch_size, set_name='test', shuffle=False)\n",
    "dataloader_validation = dataloader_val_loader[\"test\"]\n",
    "print(\"Testing  data size : \",validation_sizes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the effcientnet version2 small model base architecture and load the pretrained weights for the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.load_state_dict(torch.load(traind_resnet_model_weight,map_location=torch.device('cpu')))\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNetV2(\n",
       "  (blocks): ModuleList(\n",
       "    (0): FusedMBConvBlockV2(\n",
       "      (project_conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (project_act): SiLU()\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (1): FusedMBConvBlockV2(\n",
       "      (project_conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (project_act): SiLU()\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (2): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (4): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (5): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (6): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (8): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (9): FusedMBConvBlockV2(\n",
       "      (expand_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (project_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (10): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "      (dp_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (11): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (dp_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (12): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (dp_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (13): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (dp_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (14): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (dp_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (15): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (dp_bn): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (16): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "      (dp_bn): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (17): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (18): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (19): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (20): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (21): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (22): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (23): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (24): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (25): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "      (dp_bn): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (26): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (27): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (28): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (29): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (30): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (31): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (32): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (33): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (34): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (35): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (36): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (37): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (38): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "    (39): MBConvBlockV2(\n",
       "      (expand_conv): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (expand_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (expand_act): SiLU()\n",
       "      (dp_conv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (dp_bn): BatchNorm2d(1536, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (dp_act): SiLU()\n",
       "      (se): SqueezeExcitate(\n",
       "        (dim_reduce): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (dim_restore): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (project_conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (project_bn): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (drop_connect): DropConnect()\n",
       "    )\n",
       "  )\n",
       "  (stem_conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (stem_bn): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (stem_act): SiLU()\n",
       "  (head_conv): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (head_bn): BatchNorm2d(1280, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (head_act): SiLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (avpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=1280, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_effv2 = EfficientNetV2(model_index,\n",
    "                     in_spatial_shape=img_size,\n",
    "                     n_classes=n_classes,\n",
    "                     pretrained=pretrained,\n",
    "                     )\n",
    "model_effv2.load_state_dict(torch.load(traind_effv2_model_weight,map_location=torch.device('cpu')))\n",
    "model_effv2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vgg16 = models.vgg16_bn(pretrained=False)\n",
    "# Freeze training for all layers\n",
    "for param in model_vgg16.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# Newly created modules have require_grad=True by default\n",
    "num_features = model_vgg16.classifier[6].in_features\n",
    "features = list(model_vgg16.classifier.children())[:-1] # Remove last layer\n",
    "features.extend([nn.Linear(num_features, n_classes)]) # Add our layer with 4 outputs\n",
    "model_vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "\n",
    "model_vgg16.load_state_dict(torch.load(traind_vgg_model_weight,map_location=torch.device('cpu')))\n",
    "model_vgg16.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the dataset input data.\n",
    "Evaluate the loss of the model using cross entropy calculation.\n",
    "Measure the accuracy and F1 score metric for each class data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Measure Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FER2013 private data will be used to test the correctness of all models. A multiclass confusion matrix is utilised in addition to the overall accuracy computation to evaluate the performance of the models that have been constructed. The macro average is used to assess the model's performance, precision, recall, and F1-Score computations. The number of datasets for each label is represented by the confusion matrix's row elements. The confusion matrix's column elements, on the other hand, are numeric values that represent the projected outputs of a model on the dataset.\n",
    "\n",
    "Precision, also known as positive predictive value, is a measure that compares the number of true positives to all positive data. The number of true positives compared to the sum of all true data is referred to as recall, also known as sensitivity. The harmonic mean of precision and recall is the F1-Score. Because the macro average treats all classes equally, the class with the least data is also important.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research work, the performance of Deep Learning Models is assessed using various performance metrics such as accuracy (acc), precision (P), recall (R), specificity (S), and F1-score (F1). \n",
    "Precision: The proportion of expected positives that are true positives is measured by this parameter. As a result, true positive (TP) and false positive (FP) values are important.\n",
    "P= TP / (TP+FP)\n",
    "Recall: Recall that this is the ratio of true positives accurately classified by the model. Recall is calculated using TP and FN values.\n",
    "R = TP / (TP+FN)\n",
    "Specificity: Specificity is the proportion of true negatives (those not caused by pathology) accurately categorized by the model. Specificity is calculated using TN and FP values.\n",
    "S = TN / (TN+FP)\n",
    "F1-Score: This is a combined precision and recall measure of the model's accuracy. It's the ratio of the F1-score precision and recall metrics to their totals multiplied by two.\n",
    "\n",
    "F1 = 2 × (P×R) / (P+R)\n",
    "Accuracy: The ratio of accurately classified by the model (TP+TN) to total numbers (TP+TN+FP+FN) represents an algorithm's accuracy.\n",
    "\n",
    "ACC = (TN+TP) / (TN+FP+FN+TP)\n",
    "\n",
    "\n",
    "•\tTrue Positives (TP)\n",
    "\n",
    "•\tTrue Negatives (TN)\n",
    "\n",
    "•\tFalse positives (FP) \n",
    "\n",
    "•\tFalse Negatives (FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.56      0.58       960\n",
      "           1       0.65      0.41      0.51       111\n",
      "           2       0.57      0.45      0.50      1018\n",
      "           3       0.87      0.89      0.88      1825\n",
      "           4       0.62      0.66      0.64      1216\n",
      "           5       0.53      0.62      0.57      1139\n",
      "           6       0.78      0.77      0.78       797\n",
      "\n",
      "    accuracy                           0.68      7066\n",
      "   macro avg       0.66      0.62      0.64      7066\n",
      "weighted avg       0.68      0.68      0.67      7066\n",
      "\n",
      "Resnet Test accuaracy :  0.6769034814605152\n"
     ]
    }
   ],
   "source": [
    "val_results = eval_model(model, dataloader_validation, device, criterion=criterion)\n",
    "y_pred, y_true, loss = val_results\n",
    "Accuracy = metric_fn(y_pred, y_true)\n",
    "print(\"Resnet Test accuaracy : \",Accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficientnet V2 Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69       960\n",
      "           1       0.79      0.73      0.76       111\n",
      "           2       0.67      0.58      0.62      1018\n",
      "           3       0.92      0.93      0.93      1825\n",
      "           4       0.71      0.79      0.75      1216\n",
      "           5       0.65      0.66      0.66      1139\n",
      "           6       0.85      0.85      0.85       797\n",
      "\n",
      "    accuracy                           0.77      7066\n",
      "   macro avg       0.76      0.75      0.75      7066\n",
      "weighted avg       0.77      0.77      0.76      7066\n",
      "\n",
      "Efficientnet V2 small Test accuaracy :  0.76648740447212\n"
     ]
    }
   ],
   "source": [
    "effv2_val_results = eval_model(model_effv2, dataloader_validation, device, criterion=criterion)\n",
    "effv2_pred, effv2_true, effv2_loss = effv2_val_results\n",
    "effv2_Accuracy = metric_fn(effv2_pred, effv2_true)\n",
    "print(\"Efficientnet V2 small Test accuaracy : \",effv2_Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Experiment Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.51      0.57       960\n",
      "           1       0.61      0.37      0.46       111\n",
      "           2       0.52      0.37      0.43      1018\n",
      "           3       0.81      0.92      0.86      1825\n",
      "           4       0.58      0.72      0.64      1216\n",
      "           5       0.54      0.55      0.55      1139\n",
      "           6       0.72      0.71      0.72       797\n",
      "\n",
      "    accuracy                           0.66      7066\n",
      "   macro avg       0.63      0.59      0.60      7066\n",
      "weighted avg       0.65      0.66      0.65      7066\n",
      "\n",
      "VGG16 Test accuaracy :  0.6579394282479479\n"
     ]
    }
   ],
   "source": [
    "val_results = eval_model(model_vgg16, dataloader_validation, device, criterion=criterion)\n",
    "y_pred, y_true, loss = val_results\n",
    "Accuracy = metric_fn(y_pred, y_true)\n",
    "print(\"VGG16 Test accuaracy : \",Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above results illustrates the performance of reported models on the FER2013 dataset and models trained on this experiment. All algorithms developed for this project outperform human performance estimates. This work's model reaches state-of-the-art performance, with accuracy 77 percent higher than the previous best single-based model published.\n",
    "\n",
    "With an accuracy of 77%, the model developed in this experiment is the best single standalone model for the FER2013 dataset. Based on the results of earlier literature review experiments, the VGG model variation outperforms the other models for FER2013 data. This over-fitting issue arises from the fact that both models are complicated models that do not perform well on FER2013, owing to a lack of data as previously noted. VGG16 and RESNET show that the tiny model works effectively on FER2013.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We contributed to tackling the problem of improving facial expression recognition accuracy on FER2013 with our model, Efficientnet V2, which achieved the highest accuracy value. In the future, more effort might be done to improve results by training using the Vision Transformer model and capable hardware resources. Ensemble models, which include VGGSpinalNet and other models, can be employed in future research to increase accuracy even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\n",
    "\"Application: Facial Expression Recognition\" in Machine Learning in Computer Vision. Computational Imaging and Vision, Dordrecht: Springer, vol. 29, 2005.\n",
    "\n",
    "G.-B. Duchenne, Mécanisme de la physionomie humaine ou analyse électro-physiologique de ses différents modes de l'expression, Paris:Archives générales de médecine, vol. 1, pp. 29-47, 1862.\n",
    "\n",
    "C. Pramerdorfer and M. Kampel, Facial expression recognition using convolutional neural networks: State of the art, 2016, [online] Available: \n",
    "\n",
    "Z. Zhang, P. Luo, C.-C. Loy and X. Tang, \"Learning Social Relation Traits from Face Images\", Proc. IEEE Int. Conference on Computer Vision (ICCV), pp. 3631-3639, 2015.\n",
    "\n",
    "B.-K. Kim, S.-Y. Dong, J. Roh, G. Kim and S.-Y. Lee, \"Fusing aligned and non-aligned face information for automatic affect recognition in the wild: A deep learning approach\", Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 48-57, 2016.\n",
    "\n",
    " A. Raghuvanshi and V. Choksi, Facial Expression Recognition with Convolutional Neural Networks, 2016.\n",
    "\n",
    "A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems,pages1097–1105,2012.\n",
    "\n",
    "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385,2015.\n",
    "\n",
    "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556,2014.\n",
    "\n",
    "C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages 1–9,2015.\n",
    "\n",
    "R. Srivastava, K. Greff and J. Schmidhuber. Training Very Deep Networks. arXiv preprint arXiv:1507.06228v2,2015.\n",
    "\n",
    "S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, Nov. 1997.\n",
    "\n",
    "O. Tutsoy, F. Göngör, D. Barkana, and H. Kose, “AN EMOTION ANALYSIS ALGORITHM AND IMPLEMENTATION TO NAO HUMANOID ROBOT,” The Eurasia Proceedings of Science, Technology, Engineering & Mathematics (EPSTEM), vol. 1, pp. 316–330, Jan. 2017.\n",
    "\n",
    "F. Wang, H. Chen, L. Kong, and W. Sheng, “Real-time Facial Expression Recognition on Robot for Healthcare,” Apr. 2018, pp. 402–406. doi: 10.1109/IISR.2018.8535710.\n",
    "\n",
    "J. v. Moniaga, A. Chowanda, A. Prima, Oscar, and M. D. Tri Rizqi, “Facial Expression Recognition as Dynamic Game Balancing System,” Procedia Computer Science, vol. 135, pp. 361–368, Jan. 2018, doi: 10.1016/J.PROCS.2018.08.185.\n",
    "\n",
    "P. Ekman and W. v. Friesen, “Constants across cultures in the face and emotion,” Journal of Personality and Social Psychology, vol. 17, no. 2, pp. 124–129, Feb. 1971, doi: 10.1037/H0030377\n",
    "\n",
    "P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews, “The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression,” 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010, pp. 94–101, 2010, doi: 10.1109/CVPRW.2010.5543262.\n",
    "\n",
    "P. Ekman and E. L. Rosenberg, “What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS),” What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS), pp. 1–672, Mar. 2012, doi: 10.1093/ACPROF:OSO/9780195179644.0\n",
    "01.0001.\n",
    "\n",
    "L. Wright and N. Demeure, “Ranger21: a synergistic deep learning optimizer,” Jun.2021, Accessed: Oct. 17, 2021. [Online]. Available: \n",
    "\n",
    "M. Sokolova and G. Lapalme, “A systematic analysis of performance measures for classification tasks,” Information Processing & Management, vol. 45, no. 4, pp. 427–437, Jul. 2009, doi: 10.1016/J.IPM.2009.03.002. \n",
    "\n",
    " D. Misra, “Mish: A Self Regularized NonMonotonic Activation Function,” Aug. 2019, Accessed: Oct. 17, 2021. [Online]. Available: https://arxiv.org/abs/1908.08681v3 \n",
    "\n",
    " D. Misra, “Mish: A Self Regularized NonMonotonic Activation Function,” Aug. 2019, Accessed: Oct. 17, 2021. [Online]. Available: https://arxiv.org/abs/1908.08681v3 \n",
    "\n",
    "M. R. Zhang, J. Lucas, G. Hinton, and J. Ba, “Lookahead Optimizer: k steps forward, 1 step back,” Advances in Neural Information Processing Systems, vol. 32, Jul. 2019, Accessed: Oct. 17, 2021. [Online]. Available: https://arxiv.org/abs/1907.08610v1\n",
    "\n",
    "X. Wang et al., “ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,” Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 11133 LNCS, pp. 63–79, Sep. 2018, Accessed: Oct. 23, 2021.\n",
    " P.-L. Carrier and A. Courville, “Challenges in Representation Learning: Facial Expression Recognition Challenge,” Apr. 13, 2013. https://www.kaggle.com/c/challenges-inrepresentation-learning-facial-expressionrecognition-challenge/data (accessed Jan. 24, 2021).\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
